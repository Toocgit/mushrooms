{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Mushroom poisonous or edible classification",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "SyiDn4Ls8SaD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Objective:\n",
        "The objective is for a program to learn to accurately classify a mushroom as poisonous or edible, based on its features.\n",
        "\n",
        "A logistic regression analysis involving gradient descent will be carried out."
      ]
    },
    {
      "metadata": {
        "id": "YzeNS3fI0-iS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "# done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Hr3V6KZvapM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data used is from Kaggle: https://www.kaggle.com/uciml/mushroom-classification\n",
        "\n",
        "Detailed information about the dataset can be found at the webpage linked above.\n",
        "\n",
        "The dataset is split into two datasets:\n",
        "\n",
        "1. A training dataset (80% of the original dataset)\n",
        "2. A testing dataset (20% of the original dataset)\n",
        "\n",
        "The two datasets are loaded into pandas dataframes in the following cell."
      ]
    },
    {
      "metadata": {
        "id": "hagOKPZ19DGT",
        "colab_type": "code",
        "outputId": "6fb0c16f-cd4e-4cc9-9b55-414dca87ff31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Load training data from CSV file into pandas dataframe\n",
        "CSVTrain = pd.read_csv(\"https://raw.githubusercontent.com/Toocgit/mushrooms/master/mushrooms_train.csv\")\n",
        "\n",
        "# Load test data from CSV file into pandas dataframe\n",
        "CSVTest = pd.read_csv(\"https://raw.githubusercontent.com/Toocgit/mushrooms/master/mushrooms_test.csv\")\n",
        "\n",
        "# Verify type dataframe and dataframe dimensions\n",
        "print(type(CSVTrain), CSVTrain.shape)\n",
        "print(type(CSVTest), CSVTest.shape)\n",
        "# done"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'> (6509, 23)\n",
            "<class 'pandas.core.frame.DataFrame'> (1615, 23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "15-iJ6qyxmsr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code cell, the datasets are broken up into:\n",
        "1. Labels (mushroom classifications, poisonous / edible)\n",
        "2. Features (mushroom characteristics)\n",
        "\n",
        "There will be:\n",
        "1. Training features\n",
        "2. Training labels\n",
        "3. Testing features\n",
        "4. Testing labels"
      ]
    },
    {
      "metadata": {
        "id": "tlP6-imMMCEz",
        "colab_type": "code",
        "outputId": "694b533f-83bf-44ec-ffaf-028f4601e9ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "# Separate labels and features, for training and testing datasets\n",
        "trainLabels = CSVTrain.iloc[:, [0]]\n",
        "trainFeatures = CSVTrain.iloc[:, 1:23]\n",
        "testLabels = CSVTest.iloc[:, [0]]\n",
        "testFeatures = CSVTest.iloc[:, 1:23]\n",
        "\n",
        "# Verify type dataframe and dataframe dimensions\n",
        "print(type(trainLabels), trainLabels.shape)\n",
        "print(type(trainFeatures), trainFeatures.shape)\n",
        "print(type(testLabels), testLabels.shape)\n",
        "print(type(testFeatures), testFeatures.shape)\n",
        "# done"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'> (6509, 1)\n",
            "<class 'pandas.core.frame.DataFrame'> (6509, 22)\n",
            "<class 'pandas.core.frame.DataFrame'> (1615, 1)\n",
            "<class 'pandas.core.frame.DataFrame'> (1615, 22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tpAGudRMzDIN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The mushroom data is categorical. In the following cell, the unique categorical values are obtained for the training and testing, labels and features."
      ]
    },
    {
      "metadata": {
        "id": "hbgqsVF2Bp6i",
        "colab_type": "code",
        "outputId": "4ddd8cc1-5a6c-4c51-bed6-113430147925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "# Get unique categorical values for train and test, labels and features\n",
        "UniTrainLabels = trainLabels.nunique(axis=0, dropna=False)\n",
        "UniTrainFeatures = trainFeatures.nunique(axis=0, dropna=False)\n",
        "UniTestLabels = testLabels.nunique(axis=0, dropna=False)\n",
        "UniTestFeatures = testFeatures.nunique(axis=0, dropna=False)\n",
        "\n",
        "# Print number of unique categorical values for train and test, labels and features\n",
        "print(sum(UniTrainLabels))\n",
        "print(sum(UniTrainFeatures))\n",
        "print(sum(UniTestLabels))\n",
        "print(sum(UniTestFeatures))\n",
        "# done"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "117\n",
            "2\n",
            "83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rVCNBcz20dZ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the training and testing feature datasets, create training and testing indicator feature datasets. One indicator feature for each regular feature unique categorical value."
      ]
    },
    {
      "metadata": {
        "id": "tLFfzlwzJvTe",
        "colab_type": "code",
        "outputId": "c7b5b8c3-e06e-460f-ce97-4dd01a3cafc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Create indicator features from categorical features\n",
        "IndTrainFeatures = pd.get_dummies(trainFeatures)\n",
        "IndTestFeatures = pd.get_dummies(testFeatures)\n",
        "\n",
        "# Verify type dataframe and dimensions for train and test, indicator feature sets\n",
        "print(type(IndTrainFeatures), IndTrainFeatures.shape)\n",
        "print(type(IndTestFeatures), IndTestFeatures.shape)\n",
        "# done"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'> (6509, 117)\n",
            "<class 'pandas.core.frame.DataFrame'> (1615, 83)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3xnt8zDH06oU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code cell, indicator features are downloaded for an offline check on accuracy."
      ]
    },
    {
      "metadata": {
        "id": "AezPhRRBxkzX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Store first three records of train indicator feature set in dummiesCheck\n",
        "dummiesCheck = IndTrainFeatures.head(3)\n",
        "\n",
        "# Write dummiesCheck to CSV file and download it to a local computer directory\n",
        "with open('dummiesCheck.csv', 'w') as f:\n",
        "  f.write(dummiesCheck.to_csv())\n",
        "files.download('dummiesCheck.csv')\n",
        "# dummiesCheck.csv was reconciled with mushrooms_train.csv\n",
        "# done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xLoCoeQv1uoc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There is a discrepancy in the number of unique categorical values, found in the training features and the testing features. The discrepancy carried across into the training and testing indicator features.\n",
        "\n",
        "An offline check is done to verify that the training features include all of the unique categorical values found in the original full mushroom dataset as sourced from Kaggle.\n",
        "\n",
        "In the following code cell, a reconciliation is done between the training and testing unique indicator features."
      ]
    },
    {
      "metadata": {
        "id": "1bt6BnNqgDlH",
        "colab_type": "code",
        "outputId": "662b7d4d-27f5-446b-9f3c-a220a2915949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Get a list of the train and test, indicator feature names\n",
        "IndTrainColumns = IndTrainFeatures.columns.values.tolist()\n",
        "IndTestColumns = IndTestFeatures.columns.values.tolist()\n",
        "\n",
        "# Verify type list and number of list items, for IndTrainColumns and IndTestColumns\n",
        "print(type(IndTrainColumns), len(IndTrainColumns))\n",
        "print(type(IndTestColumns), len(IndTestColumns))\n",
        "# done"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 117\n",
            "<class 'list'> 83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CMbdJtgf3kz4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sense check the training and testing indicator feature headers:"
      ]
    },
    {
      "metadata": {
        "id": "HBdfWRw_cIN5",
        "colab_type": "code",
        "outputId": "2b566813-7cb7-4d3d-ae7f-da013a371823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Sense check values in IndTrainColumns and IndTestColumns\n",
        "print(IndTrainColumns[0:5])\n",
        "print(IndTestColumns[0:5])\n",
        "# done"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cap-shape_b', 'cap-shape_c', 'cap-shape_f', 'cap-shape_k', 'cap-shape_s']\n",
            "['cap-shape_b', 'cap-shape_c', 'cap-shape_f', 'cap-shape_k', 'cap-shape_x']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wTlJEzfq4R7o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check which indicator features are missing from the testing indicator features:"
      ]
    },
    {
      "metadata": {
        "id": "wquWcWSQcIHV",
        "colab_type": "code",
        "outputId": "df1e299b-fe88-498c-cb31-03223a01fc3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "MissingIndTestColumns = {}\n",
        "\n",
        "# For every training data indicator feature\n",
        "for IndFe in range(len(IndTrainColumns)):\n",
        "  # Check if the indicator feature is not in the test data indicator features\n",
        "  if IndTrainColumns[IndFe] not in IndTestColumns:\n",
        "    # If it is not, add it to MissingIndTestColumns\n",
        "    MissingIndTestColumns[IndFe] = IndTrainColumns[IndFe]\n",
        "\n",
        "# Check type, length and values of MissingIndTestColumns\n",
        "print(type(MissingIndTestColumns), len(MissingIndTestColumns), MissingIndTestColumns)\n",
        "# done"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'> 34 {4: 'cap-shape_s', 7: 'cap-surface_g', 10: 'cap-color_b', 16: 'cap-color_r', 17: 'cap-color_u', 22: 'odor_a', 23: 'odor_c', 25: 'odor_l', 28: 'odor_p', 38: 'gill-color_e', 40: 'gill-color_h', 41: 'gill-color_k', 45: 'gill-color_r', 46: 'gill-color_u', 54: 'stalk-root_e', 55: 'stalk-root_r', 56: 'stalk-surface-above-ring_f', 60: 'stalk-surface-below-ring_f', 64: 'stalk-color-above-ring_b', 66: 'stalk-color-above-ring_e', 67: 'stalk-color-above-ring_g', 73: 'stalk-color-below-ring_b', 75: 'stalk-color-below-ring_e', 76: 'stalk-color-below-ring_g', 91: 'ring-type_f', 92: 'ring-type_l', 96: 'spore-print-color_h', 97: 'spore-print-color_k', 100: 'spore-print-color_r', 101: 'spore-print-color_u', 104: 'population_a', 113: 'habitat_m', 115: 'habitat_u', 116: 'habitat_w'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pfuxm2tF473M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For each indicator feature missing from the testing indicator features, its position in the training indicator features is recorded. Below, a list of those positions is created."
      ]
    },
    {
      "metadata": {
        "id": "OGFMvY4hhwAi",
        "colab_type": "code",
        "outputId": "a98ae330-ab17-4d6c-8e70-9ebe6a5d49ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "# For the indicator features missing from the test data, get their position in the training data\n",
        "colIndex = list(MissingIndTestColumns.keys())\n",
        "\n",
        "# Check type, length and values of colIndex\n",
        "print(type(colIndex), len(colIndex), colIndex)\n",
        "# done"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 34 [4, 7, 10, 16, 17, 22, 23, 25, 28, 38, 40, 41, 45, 46, 54, 55, 56, 60, 64, 66, 67, 73, 75, 76, 91, 92, 96, 97, 100, 101, 104, 113, 115, 116]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DXCbLHWR53zj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Add missing indicator features into the testing indicator features dataframe and set all records to zero to indicate feature absent for each testing mushroom:"
      ]
    },
    {
      "metadata": {
        "id": "Xv-_2rSpcH9_",
        "colab_type": "code",
        "outputId": "119302ce-5beb-4f53-90e8-d5fda4e78a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# For each missing indicator feature\n",
        "for index in range(len(colIndex)):\n",
        "  # Insert the missing indicator feature into the test data, with all values as 0\n",
        "  IndTestFeatures.insert(colIndex[index], MissingIndTestColumns[colIndex[index]], 0)\n",
        "\n",
        "# Verify updated dimensions of IndTestFeatures\n",
        "IndTestFeatures.shape\n",
        "# done"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1615, 117)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "GxqyqbXQ6Yob",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Verify the same unique indicator features exist in the training and testing indicator features:"
      ]
    },
    {
      "metadata": {
        "id": "XD_w0XIcjGxV",
        "colab_type": "code",
        "outputId": "1f023c86-fcab-4c0a-aa7a-1e62c6f8bda4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Get updated list of the train and test, indicator feature names\n",
        "IndTrainColumns = IndTrainFeatures.columns.values.tolist()\n",
        "IndTestColumns = IndTestFeatures.columns.values.tolist()\n",
        "\n",
        "# Check the train indicator features and test indicator features match\n",
        "print((IndTrainColumns == IndTestColumns))\n",
        "# done"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bDNO0uU-60qI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the indicator features added into the testing indicator features dataframe, check all the values are zero:"
      ]
    },
    {
      "metadata": {
        "id": "wKQi1jTfj8IP",
        "colab_type": "code",
        "outputId": "2307c4aa-c5e7-4224-a0fd-aa652104c404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "missColValuesSum = 0\n",
        "\n",
        "# For each of the columns added to IndTestFeatures\n",
        "for num in range(len(colIndex)):\n",
        "  # Add the sum of that column's values to missColValuesSum\n",
        "  missColValuesSum += IndTestFeatures[MissingIndTestColumns[colIndex[num]]].sum()\n",
        "\n",
        "# Print missColValuesSum\n",
        "print(missColValuesSum)\n",
        "# done"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3FNXZQXdGNMa",
        "colab_type": "code",
        "outputId": "1f7c9ab1-a90b-4b5e-97b8-30590538555c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Sense check index for series and sum method\n",
        "print(type(IndTestFeatures[\"cap-shape_b\"]), IndTestFeatures[\"cap-shape_b\"].sum())\n",
        "# done"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'> 140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CouKH73J7oc2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create new label dataframes, replacing value p (poisonous) with 1, and value e (edible) with 0:"
      ]
    },
    {
      "metadata": {
        "id": "hELSXVx1jGri",
        "colab_type": "code",
        "outputId": "e30d6975-5939-4647-aeff-dc31536433c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "# For train and test labels, convert p to 1 and e to 0\n",
        "mapping = {'p': 1, 'e': 0}\n",
        "trainDataLabels = pd.DataFrame(trainLabels.iloc[:, 0].map(mapping))\n",
        "testDataLabels = pd.DataFrame(testLabels.iloc[:, 0].map(mapping))\n",
        "\n",
        "# Check type, dimensions, number unique and sample values for trainDataLabels and testDataLabels\n",
        "print(type(trainDataLabels), trainDataLabels.shape, trainDataLabels.nunique(axis=0, dropna=False))\n",
        "print(type(testDataLabels), testDataLabels.shape, testDataLabels.nunique(axis=0, dropna=False))\n",
        "print(trainDataLabels.head(3), testDataLabels.head(3))\n",
        "# done"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'> (6509, 1) class    2\n",
            "dtype: int64\n",
            "<class 'pandas.core.frame.DataFrame'> (1615, 1) class    2\n",
            "dtype: int64\n",
            "   class\n",
            "0      1\n",
            "1      0\n",
            "2      0    class\n",
            "0      1\n",
            "1      1\n",
            "2      1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-kSbYgH48DBU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create numpy ndarrays from pandas dataframes:"
      ]
    },
    {
      "metadata": {
        "id": "tVZusv5Ggip2",
        "colab_type": "code",
        "outputId": "fa40ef4e-3dab-4929-9c60-affca38d8e09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "# Create numpy ndarrays for indicator features and labels (train and test)\n",
        "npTrainFeat = IndTrainFeatures.values\n",
        "npTestFeat = IndTestFeatures.values\n",
        "npTrainLabel = trainDataLabels.values\n",
        "npTestLabel = testDataLabels.values\n",
        "\n",
        "# Verify type, shape and transpose of each ndarray\n",
        "print(type(npTrainFeat), npTrainFeat.shape, npTrainFeat.T.shape)\n",
        "print(type(npTestFeat), npTestFeat.shape, npTestFeat.T.shape)\n",
        "print(type(npTrainLabel), npTrainLabel.shape, npTrainLabel.T.shape)\n",
        "print(type(npTestLabel), npTestLabel.shape, npTestLabel.T.shape)\n",
        "# done"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (6509, 117) (117, 6509)\n",
            "<class 'numpy.ndarray'> (1615, 117) (117, 1615)\n",
            "<class 'numpy.ndarray'> (6509, 1) (1, 6509)\n",
            "<class 'numpy.ndarray'> (1615, 1) (1, 1615)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZCGBY37z9GMM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code cell, the sigmoid function is defined."
      ]
    },
    {
      "metadata": {
        "id": "YxPiJz10OQeR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I wrote this function as a part of course 1 of Andrew Ng's Deep Learning Specialisation\n",
        "\n",
        "# Define sigmoid activation function\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "    s = 1 / (1 + np.exp(-(z)))\n",
        "\n",
        "    return s\n",
        "# done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VEuG0U31B_3R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code cell, a function is defined to initialise hypothesis function parameters, a set of weights and a bias, to zero."
      ]
    },
    {
      "metadata": {
        "id": "uIGiUVQwOQVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I wrote this function as a part of course 1 of Andrew Ng's Deep Learning Specialisation\n",
        "\n",
        "# Define function to initialise hypothesis function parameters to 0\n",
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "    Argument:\n",
        "    dim -- size of the w vector we want\n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (the bias)\n",
        "    \"\"\"\n",
        "    w = np.zeros((dim, 1))\n",
        "    b = 0\n",
        "\n",
        "    # Verify dimensions of w\n",
        "    assert (w.shape == (dim, 1))\n",
        "    # Verify type of b\n",
        "    assert (isinstance(b, float) or isinstance(b, int))\n",
        "\n",
        "    return w, b\n",
        "# done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oXoDXbZaDHRl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code cell, a function is defined to:\n",
        "1. Get the number of dataset examples\n",
        "2. For every mushroom, calculate the probability of it being poisonous\n",
        "3. Calculate the cost\n",
        "4. Calculate the gradient of the cost function"
      ]
    },
    {
      "metadata": {
        "id": "DZGPE8nmOQMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I wrote this function as a part of course 1 of Andrew Ng's Deep Learning Specialisation\n",
        "\n",
        "# Calculate cost and its derivative with respect to hypothesis function parameters\n",
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient\n",
        "    Arguments:\n",
        "    w -- weights (number of features, 1)\n",
        "    b -- bias, a real number\n",
        "    X -- features of size (number of features, number of examples)\n",
        "    Y -- true \"label\" vector of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \"\"\"\n",
        "\n",
        "    # Get number of examples\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # Calculate sigmoid of all examples\n",
        "    A = sigmoid((np.dot(w.T, X) + b))\n",
        "\n",
        "    # Calculate cost\n",
        "    cost = (-(1 / m)) * np.sum((Y * np.log(A)) + ((1 - Y) * np.log(1 - A)))\n",
        "\n",
        "    # Calculate cost gradient, with respect to hypothesis function weights\n",
        "    dw = np.dot(X, (A - Y).T) / m\n",
        "\n",
        "    # Calculate cost gradient, with respect to hypothesis function bias\n",
        "    db = np.sum(A - Y) / m\n",
        "\n",
        "    # Verify cost partial derivative for each weight\n",
        "    assert (dw.shape == w.shape)\n",
        "\n",
        "    # Verify db data type\n",
        "    assert (db.dtype == float)\n",
        "\n",
        "    # Verify cost shape\n",
        "    cost = np.squeeze(cost)\n",
        "    assert (cost.shape == ())\n",
        "\n",
        "    # Create dict for dw and db\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return grads, cost\n",
        "# done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GXtZmq2nHkyK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define a function for gradient descent:"
      ]
    },
    {
      "metadata": {
        "id": "GSR2SzffOPE9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I wrote this function as a part of course 1 of Andrew Ng's Deep Learning Specialisation\n",
        "\n",
        "# Define gradient descent algorithm\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    Arguments:\n",
        "    w -- weights (number of features, 1)\n",
        "    b -- bias, a real number\n",
        "    X -- features of size (number of features, number of examples)\n",
        "    Y -- true \"label\" vector of size (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "\n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the cost with respect to the weights and bias\n",
        "    costs -- list of all the costs computed during the optimization\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    # For each iteration\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "        # Cost and gradient calculation\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "\n",
        "        # Retrieve partial derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # Parameter update rule\n",
        "        w = w - (learning_rate * dw)\n",
        "        b = b - (learning_rate * db)\n",
        "\n",
        "        # Record every hundredth cost\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "        # Print the cost every 100 iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    # Create dict of learned optimal parameters\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    # Create dict of most recent cost partial derivatives\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs\n",
        "# done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HI3RUnQsIfeO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code cell, a function is defined to classify a mushroom as poisonous or edible."
      ]
    },
    {
      "metadata": {
        "id": "yk_tUVv6OO5h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I wrote this function as a part of course 1 of Andrew Ng's Deep Learning Specialisation\n",
        "\n",
        "# Define function to predict whether mushroom is poisonous or edible\n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    Arguments:\n",
        "    w -- weights (number of features, 1)\n",
        "    b -- bias, a real number\n",
        "    X -- features of size (number of features, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a vector containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    # Get example count\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # Set prediction for all examples to 0\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    \n",
        "    # Ensure correct shape for weights\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute probability of each mushroom being poisonous\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    \n",
        "    # For each mushroom a poisonous probability was computed for...\n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # If probability A[0,i] > 0.5\n",
        "        if A[0, i] > 0.5:\n",
        "            # Update prediction p[0,i] to poisonous\n",
        "            Y_prediction[0, i] = 1\n",
        "        \n",
        "    # Verify Y_prediction shape\n",
        "    assert (Y_prediction.shape == (1, m))\n",
        "    \n",
        "    return Y_prediction\n",
        "# done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TFIjQFfII04n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define logistic regression model with gradient descent:"
      ]
    },
    {
      "metadata": {
        "id": "q2R1vXL1OOui",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I wrote this function as a part of course 1 of Andrew Ng's Deep Learning Specialisation\n",
        "\n",
        "# Define logistic regression model function\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the functions defined above\n",
        "    Arguments:\n",
        "    X_train -- training set of shape (number of features, number of examples)\n",
        "    Y_train -- training labels of shape (1, number of examples)\n",
        "    X_test -- test set of shape (number of features, number of examples)\n",
        "    Y_test -- test labels of shape (1, number of examples)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize hypothesis function parameters to zeros\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Run gradient descent\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Make predictions for test/train set examples\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # Print prediction accuracy\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\": Y_prediction_train,\n",
        "         \"w\": w,\n",
        "         \"b\": b,\n",
        "         \"learning_rate\": learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d\n",
        "# done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MOnWyrPmOtsC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code cell, the logistic regression model is executed for the first time. 2000 gradient descent steps are taken and the learning rate is 0.015.\n",
        "\n",
        "After learning some hypothesis function parameters using the training data, the program is able to accurately classify 97% of the training data mushrooms as poisonous or edible.\n",
        "\n",
        "The same percentage of correct classifications is achieved on the testing data mushrooms."
      ]
    },
    {
      "metadata": {
        "id": "sKa9khHQOOkN",
        "colab_type": "code",
        "outputId": "a8223655-2ca6-4e34-9b79-686a75ac919e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "cell_type": "code",
      "source": [
        "# First run of logistic regression model\n",
        "m1Start = time.time()\n",
        "model_one = model(npTrainFeat.T, npTrainLabel.T, npTestFeat.T, npTestLabel.T, 2000, 0.015, True)\n",
        "m1End = time.time()\n",
        "print(\"Runtime in seconds: \", (m1End - m1Start))\n",
        "# done"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.420332\n",
            "Cost after iteration 200: 0.325161\n",
            "Cost after iteration 300: 0.274102\n",
            "Cost after iteration 400: 0.240839\n",
            "Cost after iteration 500: 0.216834\n",
            "Cost after iteration 600: 0.198427\n",
            "Cost after iteration 700: 0.183739\n",
            "Cost after iteration 800: 0.171676\n",
            "Cost after iteration 900: 0.161551\n",
            "Cost after iteration 1000: 0.152903\n",
            "Cost after iteration 1100: 0.145410\n",
            "Cost after iteration 1200: 0.138839\n",
            "Cost after iteration 1300: 0.133017\n",
            "Cost after iteration 1400: 0.127813\n",
            "Cost after iteration 1500: 0.123125\n",
            "Cost after iteration 1600: 0.118873\n",
            "Cost after iteration 1700: 0.114993\n",
            "Cost after iteration 1800: 0.111434\n",
            "Cost after iteration 1900: 0.108153\n",
            "train accuracy: 97.89522200030727 %\n",
            "test accuracy: 97.46130030959752 %\n",
            "Runtime in seconds:  10.461497783660889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xbqvItwBKsI3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code cell, the logistic regression model is executed for a second time. 20000 gradient descent steps are taken and the learning rate is unchanged at 0.015.\n",
        "The training data classification accuracy is improved at 99%. The testing data classification accuracy is unchanged at 97%."
      ]
    },
    {
      "metadata": {
        "id": "rJge6CZwCAhl",
        "colab_type": "code",
        "outputId": "682ebb17-e3d6-41e7-f114-00f7f387c07d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3708
        }
      },
      "cell_type": "code",
      "source": [
        "# Second run of logistic regression model\n",
        "m2Start = time.time()\n",
        "model_two = model(npTrainFeat.T, npTrainLabel.T, npTestFeat.T, npTestLabel.T, 20000, 0.015, True)\n",
        "m2End = time.time()\n",
        "print(\"Runtime in seconds: \", (m2End - m2Start))\n",
        "# done"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.420332\n",
            "Cost after iteration 200: 0.325161\n",
            "Cost after iteration 300: 0.274102\n",
            "Cost after iteration 400: 0.240839\n",
            "Cost after iteration 500: 0.216834\n",
            "Cost after iteration 600: 0.198427\n",
            "Cost after iteration 700: 0.183739\n",
            "Cost after iteration 800: 0.171676\n",
            "Cost after iteration 900: 0.161551\n",
            "Cost after iteration 1000: 0.152903\n",
            "Cost after iteration 1100: 0.145410\n",
            "Cost after iteration 1200: 0.138839\n",
            "Cost after iteration 1300: 0.133017\n",
            "Cost after iteration 1400: 0.127813\n",
            "Cost after iteration 1500: 0.123125\n",
            "Cost after iteration 1600: 0.118873\n",
            "Cost after iteration 1700: 0.114993\n",
            "Cost after iteration 1800: 0.111434\n",
            "Cost after iteration 1900: 0.108153\n",
            "Cost after iteration 2000: 0.105115\n",
            "Cost after iteration 2100: 0.102291\n",
            "Cost after iteration 2200: 0.099657\n",
            "Cost after iteration 2300: 0.097192\n",
            "Cost after iteration 2400: 0.094879\n",
            "Cost after iteration 2500: 0.092702\n",
            "Cost after iteration 2600: 0.090648\n",
            "Cost after iteration 2700: 0.088706\n",
            "Cost after iteration 2800: 0.086866\n",
            "Cost after iteration 2900: 0.085120\n",
            "Cost after iteration 3000: 0.083459\n",
            "Cost after iteration 3100: 0.081877\n",
            "Cost after iteration 3200: 0.080367\n",
            "Cost after iteration 3300: 0.078925\n",
            "Cost after iteration 3400: 0.077546\n",
            "Cost after iteration 3500: 0.076224\n",
            "Cost after iteration 3600: 0.074956\n",
            "Cost after iteration 3700: 0.073740\n",
            "Cost after iteration 3800: 0.072570\n",
            "Cost after iteration 3900: 0.071445\n",
            "Cost after iteration 4000: 0.070362\n",
            "Cost after iteration 4100: 0.069317\n",
            "Cost after iteration 4200: 0.068310\n",
            "Cost after iteration 4300: 0.067338\n",
            "Cost after iteration 4400: 0.066398\n",
            "Cost after iteration 4500: 0.065490\n",
            "Cost after iteration 4600: 0.064612\n",
            "Cost after iteration 4700: 0.063761\n",
            "Cost after iteration 4800: 0.062937\n",
            "Cost after iteration 4900: 0.062139\n",
            "Cost after iteration 5000: 0.061364\n",
            "Cost after iteration 5100: 0.060613\n",
            "Cost after iteration 5200: 0.059883\n",
            "Cost after iteration 5300: 0.059174\n",
            "Cost after iteration 5400: 0.058485\n",
            "Cost after iteration 5500: 0.057815\n",
            "Cost after iteration 5600: 0.057164\n",
            "Cost after iteration 5700: 0.056530\n",
            "Cost after iteration 5800: 0.055912\n",
            "Cost after iteration 5900: 0.055311\n",
            "Cost after iteration 6000: 0.054725\n",
            "Cost after iteration 6100: 0.054154\n",
            "Cost after iteration 6200: 0.053596\n",
            "Cost after iteration 6300: 0.053053\n",
            "Cost after iteration 6400: 0.052523\n",
            "Cost after iteration 6500: 0.052005\n",
            "Cost after iteration 6600: 0.051499\n",
            "Cost after iteration 6700: 0.051005\n",
            "Cost after iteration 6800: 0.050523\n",
            "Cost after iteration 6900: 0.050051\n",
            "Cost after iteration 7000: 0.049590\n",
            "Cost after iteration 7100: 0.049139\n",
            "Cost after iteration 7200: 0.048697\n",
            "Cost after iteration 7300: 0.048265\n",
            "Cost after iteration 7400: 0.047842\n",
            "Cost after iteration 7500: 0.047428\n",
            "Cost after iteration 7600: 0.047023\n",
            "Cost after iteration 7700: 0.046626\n",
            "Cost after iteration 7800: 0.046237\n",
            "Cost after iteration 7900: 0.045855\n",
            "Cost after iteration 8000: 0.045481\n",
            "Cost after iteration 8100: 0.045115\n",
            "Cost after iteration 8200: 0.044755\n",
            "Cost after iteration 8300: 0.044402\n",
            "Cost after iteration 8400: 0.044056\n",
            "Cost after iteration 8500: 0.043716\n",
            "Cost after iteration 8600: 0.043383\n",
            "Cost after iteration 8700: 0.043055\n",
            "Cost after iteration 8800: 0.042734\n",
            "Cost after iteration 8900: 0.042418\n",
            "Cost after iteration 9000: 0.042108\n",
            "Cost after iteration 9100: 0.041803\n",
            "Cost after iteration 9200: 0.041503\n",
            "Cost after iteration 9300: 0.041209\n",
            "Cost after iteration 9400: 0.040919\n",
            "Cost after iteration 9500: 0.040635\n",
            "Cost after iteration 9600: 0.040355\n",
            "Cost after iteration 9700: 0.040079\n",
            "Cost after iteration 9800: 0.039809\n",
            "Cost after iteration 9900: 0.039542\n",
            "Cost after iteration 10000: 0.039280\n",
            "Cost after iteration 10100: 0.039022\n",
            "Cost after iteration 10200: 0.038768\n",
            "Cost after iteration 10300: 0.038517\n",
            "Cost after iteration 10400: 0.038271\n",
            "Cost after iteration 10500: 0.038029\n",
            "Cost after iteration 10600: 0.037790\n",
            "Cost after iteration 10700: 0.037554\n",
            "Cost after iteration 10800: 0.037323\n",
            "Cost after iteration 10900: 0.037094\n",
            "Cost after iteration 11000: 0.036869\n",
            "Cost after iteration 11100: 0.036647\n",
            "Cost after iteration 11200: 0.036429\n",
            "Cost after iteration 11300: 0.036213\n",
            "Cost after iteration 11400: 0.036000\n",
            "Cost after iteration 11500: 0.035791\n",
            "Cost after iteration 11600: 0.035584\n",
            "Cost after iteration 11700: 0.035380\n",
            "Cost after iteration 11800: 0.035179\n",
            "Cost after iteration 11900: 0.034981\n",
            "Cost after iteration 12000: 0.034785\n",
            "Cost after iteration 12100: 0.034592\n",
            "Cost after iteration 12200: 0.034401\n",
            "Cost after iteration 12300: 0.034213\n",
            "Cost after iteration 12400: 0.034028\n",
            "Cost after iteration 12500: 0.033844\n",
            "Cost after iteration 12600: 0.033663\n",
            "Cost after iteration 12700: 0.033485\n",
            "Cost after iteration 12800: 0.033308\n",
            "Cost after iteration 12900: 0.033134\n",
            "Cost after iteration 13000: 0.032962\n",
            "Cost after iteration 13100: 0.032792\n",
            "Cost after iteration 13200: 0.032624\n",
            "Cost after iteration 13300: 0.032458\n",
            "Cost after iteration 13400: 0.032294\n",
            "Cost after iteration 13500: 0.032132\n",
            "Cost after iteration 13600: 0.031972\n",
            "Cost after iteration 13700: 0.031814\n",
            "Cost after iteration 13800: 0.031658\n",
            "Cost after iteration 13900: 0.031504\n",
            "Cost after iteration 14000: 0.031351\n",
            "Cost after iteration 14100: 0.031200\n",
            "Cost after iteration 14200: 0.031051\n",
            "Cost after iteration 14300: 0.030903\n",
            "Cost after iteration 14400: 0.030757\n",
            "Cost after iteration 14500: 0.030613\n",
            "Cost after iteration 14600: 0.030470\n",
            "Cost after iteration 14700: 0.030329\n",
            "Cost after iteration 14800: 0.030189\n",
            "Cost after iteration 14900: 0.030051\n",
            "Cost after iteration 15000: 0.029914\n",
            "Cost after iteration 15100: 0.029779\n",
            "Cost after iteration 15200: 0.029646\n",
            "Cost after iteration 15300: 0.029513\n",
            "Cost after iteration 15400: 0.029382\n",
            "Cost after iteration 15500: 0.029253\n",
            "Cost after iteration 15600: 0.029124\n",
            "Cost after iteration 15700: 0.028997\n",
            "Cost after iteration 15800: 0.028872\n",
            "Cost after iteration 15900: 0.028747\n",
            "Cost after iteration 16000: 0.028624\n",
            "Cost after iteration 16100: 0.028502\n",
            "Cost after iteration 16200: 0.028381\n",
            "Cost after iteration 16300: 0.028262\n",
            "Cost after iteration 16400: 0.028143\n",
            "Cost after iteration 16500: 0.028026\n",
            "Cost after iteration 16600: 0.027910\n",
            "Cost after iteration 16700: 0.027795\n",
            "Cost after iteration 16800: 0.027681\n",
            "Cost after iteration 16900: 0.027568\n",
            "Cost after iteration 17000: 0.027457\n",
            "Cost after iteration 17100: 0.027346\n",
            "Cost after iteration 17200: 0.027236\n",
            "Cost after iteration 17300: 0.027128\n",
            "Cost after iteration 17400: 0.027020\n",
            "Cost after iteration 17500: 0.026913\n",
            "Cost after iteration 17600: 0.026808\n",
            "Cost after iteration 17700: 0.026703\n",
            "Cost after iteration 17800: 0.026599\n",
            "Cost after iteration 17900: 0.026496\n",
            "Cost after iteration 18000: 0.026394\n",
            "Cost after iteration 18100: 0.026293\n",
            "Cost after iteration 18200: 0.026193\n",
            "Cost after iteration 18300: 0.026094\n",
            "Cost after iteration 18400: 0.025996\n",
            "Cost after iteration 18500: 0.025898\n",
            "Cost after iteration 18600: 0.025801\n",
            "Cost after iteration 18700: 0.025705\n",
            "Cost after iteration 18800: 0.025610\n",
            "Cost after iteration 18900: 0.025516\n",
            "Cost after iteration 19000: 0.025423\n",
            "Cost after iteration 19100: 0.025330\n",
            "Cost after iteration 19200: 0.025238\n",
            "Cost after iteration 19300: 0.025147\n",
            "Cost after iteration 19400: 0.025056\n",
            "Cost after iteration 19500: 0.024967\n",
            "Cost after iteration 19600: 0.024878\n",
            "Cost after iteration 19700: 0.024789\n",
            "Cost after iteration 19800: 0.024702\n",
            "Cost after iteration 19900: 0.024615\n",
            "train accuracy: 99.81563988323859 %\n",
            "test accuracy: 97.3374613003096 %\n",
            "Runtime in seconds:  100.01919770240784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aBxkPNgyTfwv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A third run of the logistic regression model is executed in the following code cell. 2000 gradient descent steps are taken and the learning rate is 0.5.\n",
        "The program is able to accurately classify 99% of the training mushrooms and 98% of the testing mushrooms."
      ]
    },
    {
      "metadata": {
        "id": "XgxsGAsFWiG8",
        "colab_type": "code",
        "outputId": "64542ac2-4f1c-4f82-eec0-e05f1e7e5429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "cell_type": "code",
      "source": [
        "# Third run of logistic regression model\n",
        "m3Start = time.time()\n",
        "model_three = model(npTrainFeat.T, npTrainLabel.T, npTestFeat.T, npTestLabel.T, 2000, 0.5, True)\n",
        "m3End = time.time()\n",
        "print(\"Runtime in seconds: \", (m3End - m3Start))\n",
        "# done"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.078084\n",
            "Cost after iteration 200: 0.051024\n",
            "Cost after iteration 300: 0.039199\n",
            "Cost after iteration 400: 0.032351\n",
            "Cost after iteration 500: 0.027796\n",
            "Cost after iteration 600: 0.024501\n",
            "Cost after iteration 700: 0.021978\n",
            "Cost after iteration 800: 0.019969\n",
            "Cost after iteration 900: 0.018319\n",
            "Cost after iteration 1000: 0.016933\n",
            "Cost after iteration 1100: 0.015748\n",
            "Cost after iteration 1200: 0.014721\n",
            "Cost after iteration 1300: 0.013819\n",
            "Cost after iteration 1400: 0.013021\n",
            "Cost after iteration 1500: 0.012309\n",
            "Cost after iteration 1600: 0.011670\n",
            "Cost after iteration 1700: 0.011092\n",
            "Cost after iteration 1800: 0.010568\n",
            "Cost after iteration 1900: 0.010090\n",
            "train accuracy: 99.89245659855584 %\n",
            "test accuracy: 98.45201238390092 %\n",
            "Runtime in seconds:  9.667662143707275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xyctc806OOXM",
        "colab_type": "code",
        "outputId": "cc3a23d9-07e2-4d53-a9c7-e1c8e1cbb6e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "# Check for expected keys in model output dicts\n",
        "print(model_one.keys())\n",
        "print(model_two.keys())\n",
        "print(model_three.keys())\n",
        "# done"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['costs', 'Y_prediction_test', 'Y_prediction_train', 'w', 'b', 'learning_rate', 'num_iterations'])\n",
            "dict_keys(['costs', 'Y_prediction_test', 'Y_prediction_train', 'w', 'b', 'learning_rate', 'num_iterations'])\n",
            "dict_keys(['costs', 'Y_prediction_test', 'Y_prediction_train', 'w', 'b', 'learning_rate', 'num_iterations'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z93DK6IRT3fl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An interesting observation in the following code cell output is that the learned bias of the first and third model act to classify mushrooms as poisonous whereas the learned bias of the second model would act to classify mushrooms as edible.\n",
        "\n",
        "This variability requires further investigation, outside the scope of this project."
      ]
    },
    {
      "metadata": {
        "id": "JTMWnVFMtZ2f",
        "colab_type": "code",
        "outputId": "2c4a8724-4c78-492f-e0ee-7a57c8d4b7f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Check value of learned bias for each model\n",
        "print(model_one['b'])\n",
        "print(model_two['b'])\n",
        "print(model_three['b'])\n",
        "# done"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.010509180779368665\n",
            "-0.0019200792399652418\n",
            "0.007252501284600106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u-fDyqXP5oue",
        "colab_type": "code",
        "outputId": "b5d1a1ba-bd18-4fff-ad2d-43f4e40296f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Check data type of model learned weights\n",
        "type(model_one['w'])\n",
        "# done"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "LYlSzvrUD2fG",
        "colab_type": "code",
        "outputId": "b33ee1b5-2404-42fe-b7f7-922c085a4198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Create dataframe for each model learned weights\n",
        "m1weights = pd.DataFrame.from_dict(model_one['w'])\n",
        "m2weights = pd.DataFrame.from_dict(model_two['w'])\n",
        "m3weights = pd.DataFrame.from_dict(model_three['w'])\n",
        "\n",
        "# Check for dataframe type and shape\n",
        "print(type(m1weights), m1weights.shape)\n",
        "print(type(m2weights), m2weights.shape)\n",
        "print(type(m3weights), m3weights.shape)\n",
        "# done"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'> (117, 1)\n",
            "<class 'pandas.core.frame.DataFrame'> (117, 1)\n",
            "<class 'pandas.core.frame.DataFrame'> (117, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ekibuezZEGBK",
        "colab_type": "code",
        "outputId": "408da48a-7943-4a11-dac4-cabca55fc506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "# Sense check weight values\n",
        "print(m1weights.head(3))\n",
        "print(m2weights.head(3))\n",
        "print(m3weights.head(3))\n",
        "# done"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          0\n",
            "0 -0.056552\n",
            "1  0.006831\n",
            "2  0.081142\n",
            "          0\n",
            "0  0.213989\n",
            "1  0.073459\n",
            "2  0.093557\n",
            "          0\n",
            "0  0.542081\n",
            "1  0.207228\n",
            "2  0.026127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ngZ54B0NZdXa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following two code cells involve a check on the greatest learned weight for each model and as a consequence, the most significant feature for a poisonous classification. All three models point to indicator feature 24, see further below for what feature that is."
      ]
    },
    {
      "metadata": {
        "id": "mbqfKXm3EI84",
        "colab_type": "code",
        "outputId": "db8d6fe6-fd3e-4b7a-b233-9f8cb3a918d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Find greatest weight for each model\n",
        "print(m1weights.iloc[:, 0].idxmax(), m1weights.iloc[:, 0].max())\n",
        "print(m2weights.iloc[:, 0].idxmax(), m2weights.iloc[:, 0].max())\n",
        "print(m3weights.iloc[:, 0].idxmax(), m3weights.iloc[:, 0].max())\n",
        "# done"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24 0.916698404805886\n",
            "24 1.8447427687724076\n",
            "24 2.5064639850291943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FO_Vx7qIFU2o",
        "colab_type": "code",
        "outputId": "17b9418b-7a93-40de-df96-865fe4603658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Sense check last code cell\n",
        "print(m1weights.iloc[24, 0])\n",
        "print(m2weights.iloc[24, 0])\n",
        "print(m3weights.iloc[24, 0])\n",
        "# done"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.916698404805886\n",
            "1.8447427687724076\n",
            "2.5064639850291943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UF3HT-KlBGsw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dataframes of model costs\n",
        "m1costs = pd.DataFrame.from_dict(model_one['costs'])\n",
        "m2costs = pd.DataFrame.from_dict(model_two['costs'])\n",
        "m3costs = pd.DataFrame.from_dict(model_three['costs'])\n",
        "# done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gesggCuSb-ci",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code cell, the 'learning curve' for each model is plot. The y axis has the cost function output and the x axis has every one hundredth gradient descent step. The model three plot is where the cost minimises to convergence very quickly, after what looks like about 100 gradient descent steps. Model three used the greatest learning rate."
      ]
    },
    {
      "metadata": {
        "id": "zW6tjiEOBGQf",
        "colab_type": "code",
        "outputId": "4d381bb0-d8a9-4b31-f138-9206c56d7a35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1025
        }
      },
      "cell_type": "code",
      "source": [
        "# Plot learning curves\n",
        "m1costs.plot.line()\n",
        "m2costs.plot.line()\n",
        "m3costs.plot.line()\n",
        "# done"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f36caa16d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwkRERCL5/+g0EB3PY9O3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYQBAEgSQhgAgCT5/6s4uNXE3K1WAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vqnDAfWEXDdA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code cell, a check on each model's five greatest learned weights is done and, as a result, the five most significant features that contribute to a poisonous classification.\n",
        "\n",
        "Interestingly, there is variability across the models as to what the five most significant features to a poisonous classification are. Further investigation is required, outside the scope of this project."
      ]
    },
    {
      "metadata": {
        "id": "Cfh7jsBzn-SV",
        "colab_type": "code",
        "outputId": "b9b8188f-9db9-4a7f-d2c9-b0301f9c4021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "cell_type": "code",
      "source": [
        "# Check five greatest weights per model\n",
        "print(m1weights.nlargest(5, 0))\n",
        "print(m2weights.nlargest(5, 0))\n",
        "print(m3weights.nlargest(5, 0))\n",
        "# done"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            0\n",
            "24   0.916698\n",
            "36   0.747348\n",
            "96   0.673443\n",
            "108  0.585371\n",
            "57   0.572514\n",
            "           0\n",
            "24  1.844743\n",
            "52  1.528609\n",
            "28  1.452484\n",
            "36  1.379474\n",
            "23  1.235566\n",
            "            0\n",
            "24   2.506464\n",
            "52   2.263623\n",
            "28   2.092444\n",
            "23   2.030237\n",
            "100  1.957834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RYuVq8q4g-Vd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code cell, a check on each model's five smallest learned weights is performed and, as a result, which five features per model are most significant to an edible classification.\n",
        "\n",
        "Again there is variability across the models as to what the five most significant features to an edible classification are, further investigation is required."
      ]
    },
    {
      "metadata": {
        "id": "RZem839voKRw",
        "colab_type": "code",
        "outputId": "978e4f12-387b-4efd-bf19-9d6f03fa2f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "cell_type": "code",
      "source": [
        "# Check five smallest weights per model\n",
        "print(m1weights.nsmallest(5, 0))\n",
        "print(m2weights.nsmallest(5, 0))\n",
        "print(m3weights.nsmallest(5, 0))\n",
        "# done"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           0\n",
            "27 -1.325841\n",
            "35 -0.736839\n",
            "98 -0.515929\n",
            "58 -0.490938\n",
            "97 -0.474107\n",
            "           0\n",
            "27 -2.812242\n",
            "35 -1.381394\n",
            "22 -1.260489\n",
            "25 -1.260489\n",
            "53 -1.109363\n",
            "           0\n",
            "27 -3.708213\n",
            "25 -2.061147\n",
            "22 -2.061147\n",
            "35 -1.902235\n",
            "53 -1.456662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mwuLX35zsYHd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next six code cells involve checks on indexes."
      ]
    },
    {
      "metadata": {
        "id": "FCmV6pjNyQNK",
        "colab_type": "code",
        "outputId": "0bf3f0c6-ceb3-43cc-c0e7-7b5c9b1e602e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Check shape and index of weights\n",
        "print(m1weights.shape)\n",
        "print(m1weights.index)\n",
        "# done"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(117, 1)\n",
            "RangeIndex(start=0, stop=117, step=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XfvQ_mHyr59V",
        "colab_type": "code",
        "outputId": "6c068bbd-f83d-40a1-ff68-e0f6ba6a6615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        }
      },
      "cell_type": "code",
      "source": [
        "# Verify index 117 is out of range\n",
        "m1weights.iloc[117, 0]\n",
        "# done"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-176fd5a44d11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm1weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m117\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1737\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1738\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Too many indexers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 raise ValueError(\"Location based indexing can only have \"\n\u001b[1;32m    206\u001b[0m                                  \u001b[0;34m\"[{types}] types\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1670\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_valid_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_is_valid_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1713\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eCCTyKNur_R3",
        "colab_type": "code",
        "outputId": "81c3743c-f549-44a0-e2f3-4c905145cb82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Verify index 116 in range\n",
        "m1weights.iloc[116, 0]\n",
        "# done"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.15056092995012443"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "aLBjcjIEo8eE",
        "colab_type": "code",
        "outputId": "420b7142-4512-4a6f-bc2b-da85993dc7dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Check length of list IndTrainColumns\n",
        "len(IndTrainColumns)\n",
        "# done"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "117"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "nhpu0MU1sFcC",
        "colab_type": "code",
        "outputId": "c6636cba-5b58-4e70-a3ca-e7d995519784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "cell_type": "code",
      "source": [
        "# Check index 117 out of range\n",
        "IndTrainColumns[117]\n",
        "# done"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-078b4a14b843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mIndTrainColumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m117\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wgssa0ossJht",
        "colab_type": "code",
        "outputId": "3c625998-b61f-4961-8b2b-b5f52d15d76e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Check index 116 in range\n",
        "IndTrainColumns[116]\n",
        "# done"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'habitat_w'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "zhaeQ-wUxXXo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All three models learned weights which indicate:\n",
        "1. Mushroom odor of f (foul) is the most significant feature / common characteristic of poisonous mushrooms.\n",
        "2. Mushroom odor of n (none) is the most significant feature / common characteristic of edible mushrooms."
      ]
    },
    {
      "metadata": {
        "id": "9OOi1kiWpUEZ",
        "colab_type": "code",
        "outputId": "0f351abc-9d21-4be5-ca60-98c5b5c06c1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Check what are indicator features [24] and [27]\n",
        "print(IndTrainColumns[24])\n",
        "print(IndTrainColumns[27])\n",
        "# done"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odor_f\n",
            "odor_n\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}